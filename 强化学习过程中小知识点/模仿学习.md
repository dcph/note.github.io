传统强化学习通过与环境进行交互来得到累计奖励从而求出最优策略，这种方法在稀疏奖励中应用效果有限，因为此时得到的奖励少，所以需要经过较长时间才能对模型进行更进。
模仿学习Imitation Learning，可以解决这种稀疏奖励和多步奖励的问题，他将人类玩家的决策数据编辑成为动作数据集D，然后通过回归学习来得到较好的拟合，这种方法和生成对抗网络以及自编码器比较像。
都是学习已有的结果然后拟合模型，并通过模型的泛化功能来得到下次不同状态时候原有拟合状态可能出现的动作。这个任务也叫做行为克隆（Behavior Cloning），即作为监督学习的模仿学习。

但是这种方法并不是能够完全拟合的，这里存在复合误差（compounding errors），训练好的策略模型执行的轨迹和训练轨迹的误差会随时间的增加而越变越大，方法主要有两处困难。

collection expect demonstrations：费时费力，还需要大量数据样本进行监督学习训练。此外，只能收集到好的示范，会导致事故的示范很难收集。

Error accumulates: 模仿不可能精确地复制示范地动作。又由于强化学习是序列决策问题，这跟普通的mnist图像分类不同，误差是会累积的

解决方法一般有数据增强之类的，但是效果也有限，因为在提供数据的时候不能保证数据可靠的量有多少，也很难保证有没有遗漏，有没有冗余数据。
比如石佛棋力很高，并且在复杂局面习惯长考，但是模型难以学到棋力，反倒学会长考就很麻烦。

另一种模仿学习是逆强化学习，不再是学习最优输出，假设专家数据为最优输出，通过模型学习最优奖励，这是和生成对抗网络最接近的一种方式。
我们通过专家数据，去学习一个奖励函数，再通过这个奖励函数去生成模型的轨迹。至于如何去学习这个奖励函数，要保证专家数据获得的奖励一定要比模型生成的数据的奖励要多，这样奖励函数就会强迫模型轨迹拟合专家数据。
这里，通过奖励函数生成轨迹的过程就像是generater，而奖励函数的生成过程就像是discriminater，通过对抗学习的思想进行更新。不过这一种方法计算量很大。

模仿学习的存在要求有专家数据或者理想模型的存在，如果是从零开始自我学习那么就不可用。
