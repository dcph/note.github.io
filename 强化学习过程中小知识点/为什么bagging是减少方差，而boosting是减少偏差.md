bagging即Bootstrap Aggregating，意思就是独立地同分布取样（Bootstrap）然后在每组取样结果训练出来的模型取平均（Aggregating），
所以是降低模型的variance，是一种并行算法，举例比如说森林算法，本质上是通过模型平均的方式来减少方差，但是一般竞赛用的多，不提倡
在论文中使用，因为哪怕是很弱的模型在经过平均后都会有提高，所以不提倡用作基准，使用训练资源多，对于方差大，训练快捷的模型实用。

Boosting 则是迭代算法，用于将多个弱分类器经过组合迭代生成一个强分类器，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所
以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行，例子比如Adaptive Boosting.
