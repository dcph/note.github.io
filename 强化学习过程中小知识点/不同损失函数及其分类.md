损失函数的作用是衡量几个结构或者几个高维空间分布之间的差异性，深度学习中使用的损失函数分为回归损失函数和分类损失函数两大类，深度学习有三大类工作，回归，分类，聚类，其中聚类是无监督的，常见回归损失函数和分类损失函数如下：
 ![Image text](/强化学习过程中小知识点/images/UDO7`PEQ)C33%X9MD16YESH.png)
均方误差MSE：最常用的回归损失函数但是当数据中有异常点时受影响较大，这时可用较为稳定的MAE，MAE问题在于他的梯度恒定，不利于梯度下降。![Image text](/强化学习过程中小知识点/images/27`6U)Z~H}}WS1K6`W)61~0.png)
![Image text](/强化学习过程中小知识点/images/I6UYRY~DXE]N@XNRB]`PAVE.png)
Huber损失：平滑的平均绝对误差，也是回归损失函数，本质上，Huber损失是绝对误差，只是在误差很小时，就变为平方误差。误差降到多小时变为二次误差由超参数δ（delta）来控制。当Huber损失在[0-δ,0+δ]之间时，等价为MSE，而在[-∞,δ]和[δ,+∞]时为MAE它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点，但是问题在于它增加了一个超参数。![Image text](/强化学习过程中小知识点/images/ZOU2KP5M6U117CPYKW8F`(E.png)
Log-cosh函数：应用于回归问题中，比L2更平滑，计算方式是预测误差的双曲余弦的对数，基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，Log-cosh二阶处处可微，二阶处处可微即意味着可以使用牛顿法进行优化。缺点是如误差很大的话，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。![Image text](/强化学习过程中小知识点/images/E]94S9~OY%H{J~QHYY`K0%T.png)
0-1 Loss：最简单的一种二分类损失函数，结果正确为1，否则为0
Cross Entropy Loss：分类中最常用的交叉熵损失函数，常与softmax激活函数一起使用![Image text](/强化学习过程中小知识点/images/3RFW9G3HZETD1L(JDGVIB(3.png )
